{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Natural Language Processing\r\n",
    "NLP is a machine learning field that understand the words in a sentence individually and the context behind using those words. Some of the common NLP tasks are - reviewing and classifying whole sentences, identifying each word as part of speech(noun, verb , adjective, entity), generating text with masked words, extracting information from the context, transalting or summarizing the text. \r\n",
    "\r\n",
    "### Transformers \r\n",
    "One of the mdoels used for solving NLP tasks is the transformers. I will be using the transformers library from Huggingface.\r\n",
    "The pipeline API function will connect the pre-trained model with pre-processed inputs and post-prpcessing predictions to directly output the sentiment label of the sentence with accuracy score. For example- Zero-shot classification pipeline allows in annotating the text and classify the sentence into sentimental labels. Text generation pipeline will auto-complete the sentence by generatinhg the remaining text with a provided prompt. Some of the other examples include - Named entity recognition, question answering, summarization, transalation\r\n",
    "\r\n",
    "### Categories of pre-trained transformers models\r\n",
    "- GPT-like (auto-regressive)\r\n",
    "- BERT-like (auto-encoding)\r\n",
    "- BART/T5-like (sequence-to-sequence)\r\n",
    "\r\n",
    "### Transformer architecture\r\n",
    "-\tEncoder: Encoder encode the input into numerical representation of each word.\r\n",
    "-\tDecoder: Decoder uses the encoder’s representation along with other inputs to generate a target sequence \r\n",
    "-\tEncoder-only models: Bi-directional, for sentiment analysis (sentence classification) and named entity recognition. Ex- BERT, RoBERTa, ALBERT\r\n",
    "-\tDecoder-only models: unidirectional, for text generation, Ex – CTRL, GPT, GPT-2, Transformer XL\r\n",
    "-\tEncoder-Decoder or sequence-to-sequence models: translation or summarization, Ex – BART, mBART, Marian, T5\r\n",
    "\r\n",
    "## Using Transformers\r\n",
    "Stage 1 (tokenizer)  Stage 2 (Model)  Stage 3 (post processing)\r\n",
    "\r\n",
    "Raw text -->     Input ID     -->       Logits (not probabilities)    -->    Predictions\r\n",
    "Tokenizer: \r\n",
    "-\tsplit the input into words, subwords, symbols referred as tokens\r\n",
    "-\tMapping each token to an integer\r\n",
    "-\t“AutoTokenizer” class and “from_pretrained” method\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import pandas as pd\r\n",
    "# load dataset\r\n",
    "df = pd.read_csv(r\"C:\\Users\\minnu\\Desktop\\Sentiment-analysis-using-NLP\\dataset_exploration\\all-data.csv\", names = ['label', 'sentences'], sep=',', encoding='latin-1')\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                          sentences\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df_2 = df.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Preprocessing with Tokenizer\r\n",
    "from transformers import AutoTokenizer\r\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "raw_inputs = [df_2['sentences'][2],df_2['sentences'][4]] \r\n",
    "inputs = tokenizer(raw_inputs, padding= True, truncation = True, return_tensors = 'pt')\r\n",
    "print(inputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'input_ids': tensor([[  101,  1996,  2248,  4816,  3068,  2194,  3449, 19800,  4160,  2038,\n",
      "          4201,  2125, 15295,  1997,  5126,  2013,  2049, 21169,  4322,  1025,\n",
      "         10043,  2000,  3041,  3913, 27475,  1996,  2194, 11016,  1996,  6938,\n",
      "          1997,  2049,  2436,  3667,  1010,  1996,  3679,  2695, 14428,  2229,\n",
      "          2988,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0],\n",
      "        [  101,  2429,  2000,  1996,  2194,  1005,  1055,  7172,  5656,  2005,\n",
      "          1996,  2086,  2268,  1011,  2262,  1010, 19021,  8059,  7889,  1037,\n",
      "          2146,  1011,  2744,  5658,  4341,  3930,  1999,  1996,  2846,  1997,\n",
      "          2322,  1003,  1011,  2871,  1003,  2007,  2019,  4082,  5618,  7785,\n",
      "          1997,  2184,  1003,  1011,  2322,  1003,  1997,  5658,  4341,  1012,\n",
      "           102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]])}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Going through the model\r\n",
    "from transformers import AutoModel   # AutoModel instantiate the model from checkpoint\r\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\r\n",
    "model = AutoModel.from_pretrained(checkpoint)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# output displaying the batch size, sequence length, hidden vector dimension\r\n",
    "outputs = model(**inputs)\r\n",
    "print(outputs.last_hidden_state.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 51, 768])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Model heads: making sense out of numbers\r\n",
    "from transformers import AutoModelForSequenceClassification\r\n",
    "\r\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\r\n",
    "outputs = model(**inputs)\r\n",
    "print(outputs.logits)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 1.7797, -1.6082],\n",
      "        [-0.9567,  0.8728]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import torch\r\n",
    "\r\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim =-1)\r\n",
    "print(predictions)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.9673, 0.0327],\n",
      "        [0.1383, 0.8617]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## From tokenizer to model\r\n",
    "1. Intialize the BERT model "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# The AutoConfig API allows to instantiate the configuration \r\n",
    "# of a pretrained model from any checkpoint. It contains all the information needed \r\n",
    "# to load the model\r\n",
    "'''\r\n",
    "from transformers import BertConfig, BertModel \r\n",
    "config = BertConfig()\r\n",
    "model = BertModel(config)   # Model is randomly intialized\r\n",
    "'''\r\n",
    "from transformers import BertModel \r\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. **Tokenizers**: Tokenizers translate raw text inputs into numerical data that can be processed by the model. It could be either word-based, character-based or subword tokenization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from transformers import AutoTokenizer\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\r\n",
    "\r\n",
    "sequence = df_2[\"sentences\"][2]\r\n",
    "tokens = tokenizer.tokenize(sequence)\r\n",
    "\r\n",
    "print(tokens)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['The', 'international', 'electronic', 'industry', 'company', 'El', '##cote', '##q', 'has', 'laid', 'off', 'tens', 'of', 'employees', 'from', 'its', 'Tallinn', 'facility', ';', 'contrary', 'to', 'earlier', 'lay', '##offs', 'the', 'company', 'contracted', 'the', 'ranks', 'of', 'its', 'office', 'workers', ',', 'the', 'daily', 'Post', '##ime', '##es', 'reported', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# From tokens to input IDs\r\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
    "print(ids)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1109, 1835, 4828, 2380, 1419, 2896, 21596, 4426, 1144, 3390, 1228, 17265, 1104, 4570, 1121, 1157, 23277, 3695, 132, 11565, 1106, 2206, 3191, 18438, 1103, 1419, 11058, 1103, 6496, 1104, 1157, 1701, 3239, 117, 1103, 3828, 3799, 10453, 1279, 2103, 119]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Decode\r\n",
    "decode_string = tokenizer.decode(ids)\r\n",
    "print(decode_string)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility ; contrary to earlier layoffs the company contracted the ranks of its office workers, the daily Postimees reported.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. **Handling multiple sequences**: \r\n",
    "Model expects a batch of inputs, i.e sending multiple sentences through the model all at once. Padding makes sure all the sentences have the same length by adding a special word called padding token to the sentences with fewer values.  To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask. We can truncate the lengths of the sequences by specifying max_sequence_length parameter. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**After understanding how tokenizer and pretrained model works on few sequences, now it's time to fine-tune the pretrained model on the whole dataset.** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine-tuning a pretrained model \r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('env': venv)"
  },
  "interpreter": {
   "hash": "11f3fe90e44092b39ef86a8801812c8b85dc0f0fce196ca0f6382945bc24a6e3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}